{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практическое задание к уроку 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31962, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                              tweet\n",
       "id                                                          \n",
       "1       0   @user when a father is dysfunctional and is s...\n",
       "2       0  @user @user thanks for #lyft credit i can't us...\n",
       "3       0                                bihday your majesty\n",
       "4       0  #model   i love u take with u all the time in ...\n",
       "5       0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('./train.csv', index_col='id')\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описание датасета:\n",
    "The objective of this task is to detect hate speech in tweets.  \n",
    "For the sake of simplicity, we say a tweet contains hate speech  \n",
    "if it has a racist or sexist sentiment associated with it.  \n",
    "So, the task is to classify racist or sexist tweets from other tweets.  \n",
    "  \n",
    "Formally, given a training sample of tweets and labels, where label '1'  \n",
    "denotes the tweet is racist/sexist and label '0' denotes the tweet is  \n",
    "not racist/sexist, your objective is to predict the labels on the test dataset.  \n",
    "  \n",
    "Таким образом, нам нужно будет искать твиты, которые содержат  \n",
    "расистский или сексистский смысл.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17197, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31963</th>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31964</th>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31965</th>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31966</th>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31967</th>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet\n",
       "id                                                      \n",
       "31963  #studiolife #aislife #requires #passion #dedic...\n",
       "31964   @user #white #supremacists want everyone to s...\n",
       "31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "31966  is the hp and the cursed child book up for res...\n",
       "31967    3rd #bihday to my amazing, hilarious #nephew..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('./test.csv', index_col='id')\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как тестовые данные не содержат меток, то будем использовать только  \n",
    "трейн для обучения и валидации, чтобы можно было оценить качество модели.  \n",
    "Посмотрим на баланс классов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    29720\n",
       "1     2242\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.256021409455842"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()[0] / df_train['label'].value_counts()[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как часто бывает в подобных задачах, мы имеем большой дисбаланс классов.  \n",
    "Забегая вперёд, мной была предпринята попытка устранить его путём классического  \n",
    "оверсэмплинга, где обучающий датасет был увеличен сэмплами из себя же, чтобы уравнять  \n",
    "количество объектов обоих классов. Данный подход привёл к очень быстрому переобучению  \n",
    "ввиду и так малого размера исходного датасета, а упрощать модель было уже некуда.  \n",
    "В итоге, от идеи оверсэмплинга пришлось отказаться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем разбивку на трейн и валидацию:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25569, 2), (6393, 2))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_val = train_test_split(df_train, \n",
    "                                    test_size=0.2, \n",
    "                                    random_state=RANDOM_STATE, \n",
    "                                    stratify=df_train['label'])\n",
    "\n",
    "df_train.shape, df_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем подготовку текстов:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/maxim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/maxim/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = set(punctuation)\n",
    "# Не будем очищать текст от апострофов, заменим их потом на пробелы,\n",
    "# т.к. встроенные в nltk английские стопслова и так потом отфильтруют лишнее\n",
    "puncts = puncts - {\"'\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(txt):\n",
    "    txt = str(txt)\n",
    "    txt = ''.join(char for char in txt if char not in puncts) # очистка от пунктуации\n",
    "    txt = txt.replace(\"'\", \" \")\n",
    "    txt = txt.lower().split()\n",
    "    txt = [word for word in txt if word.isalpha()] # очистка от символов и цифр\n",
    "    txt = [lemmatizer.lemmatize(word) for word in txt] # лемматизация\n",
    "    txt = [word for word in txt if word not in stopwords.words('english')] # очистка от стопслов\n",
    "    return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25569/25569 [00:39<00:00, 648.98it/s]\n",
      "100%|██████████| 6393/6393 [00:09<00:00, 664.01it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "df_train['tweet'] = df_train['tweet'].progress_apply(preprocess_text)\n",
    "df_val['tweet'] = df_val['tweet'].progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14553</th>\n",
       "      <td>0</td>\n",
       "      <td>user amazing wait see going cantwait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>0</td>\n",
       "      <td>wait new user trailer gamer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12125</th>\n",
       "      <td>0</td>\n",
       "      <td>thriving iam positive affirmation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6326</th>\n",
       "      <td>0</td>\n",
       "      <td>happy new user book lil upset page faded user ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>0</td>\n",
       "      <td>arrive cold rainy english noh first time back ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                              tweet\n",
       "id                                                             \n",
       "14553      0               user amazing wait see going cantwait\n",
       "2563       0                        wait new user trailer gamer\n",
       "12125      0                  thriving iam positive affirmation\n",
       "6326       0  happy new user book lil upset page faded user ...\n",
       "3996       0  arrive cold rainy english noh first time back ..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим общий корпус текста:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = ''.join(df_train['tweet'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем токенизацию:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user', 'amazing', 'wait', 'see', 'going']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(train_corpus)\n",
    "tokens[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим словарь:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 2000\n",
    "MAX_LEN = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = FreqDist(tokens)\n",
    "tokens_top = [items[0] for items in dist.most_common(MAX_WORDS - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user', 'day', 'love', 'u', 'amp', 'like', 'life', 'happy', 'get', 'wa']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_top[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {word: count for count, word in dict(enumerate(tokens_top, 1)).items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переведём твиты в набор индексов, добавим паддинг:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(txt, maxlen):\n",
    "    result = []\n",
    "    tokens = word_tokenize(txt)\n",
    "    for word in tokens:\n",
    "        if word in vocabulary:\n",
    "            result.append(vocabulary[word])\n",
    "\n",
    "    padding = [0] * (maxlen-len(result))\n",
    "    return result[-maxlen:] + padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25569, 20), (6393, 20))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array([text_to_sequence(txt, MAX_LEN) for txt in df_train['tweet'].values])\n",
    "X_val = np.array([text_to_sequence(txt, MAX_LEN) for txt in df_val['tweet'].values])\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оригинальная строка: found beautiful one bedroom double stall garage patio amp huge kitchen signed lease wait move\n",
      "Обработанная строка: [ 172   51   19 1233    5  777 1537 1538   68  694    0    0    0    0\n",
      "    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Оригинальная строка: {df_train['tweet'].iloc[5]}\")\n",
    "print(f\"Обработанная строка: {X_train[5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем свёрточную нейросеть:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size=2000, embedding_dim=128, out_channel=64, num_classes=1, threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0) \n",
    "        self.conv_1 = nn.Conv1d(embedding_dim, out_channel, kernel_size=3, padding='same') \n",
    "        self.bn1 = nn.BatchNorm1d(out_channel)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_1 = nn.Linear(out_channel, num_classes)\n",
    "        self.dp1d = nn.Dropout1d(0.5)\n",
    "        self.dp = nn.Dropout(0.5)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):     # Для понимания обозначим размеры входных данных на каждом слое,\n",
    "                              # используя гиперпараметры по умолчанию и max_len=20\n",
    "        x = self.embedding(x) # (1, 20) -> (1, 20, 128)       \n",
    "        x = x.permute(0, 2, 1) # (1, 20, 128) -> (1, 128, 20)\n",
    "        x = self.conv_1(x) # (1, 128, 20) -> (1, 64, 20)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dp1d(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x) # (1, 64, 20) -> (1, 64, 10)\n",
    "        \n",
    "        x = torch.max(x, axis=2).values # (1, 64, 10) -> (1, 64)\n",
    "        x = self.dp(x)\n",
    "        x = self.linear_1(x) # (1, 64) -> (1, 1)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = torch.IntTensor(x).to(device)\n",
    "        x = self.forward(x)\n",
    "        x = torch.squeeze((x > self.threshold).int())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим структуру сети:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Net                                      [1, 1]                    --\n",
       "├─Embedding: 1-1                         [1, 20, 128]              256,000\n",
       "├─Conv1d: 1-2                            [1, 64, 20]               24,640\n",
       "├─BatchNorm1d: 1-3                       [1, 64, 20]               128\n",
       "├─Dropout1d: 1-4                         [1, 64, 20]               --\n",
       "├─ReLU: 1-5                              [1, 64, 20]               --\n",
       "├─MaxPool1d: 1-6                         [1, 64, 10]               --\n",
       "├─Dropout: 1-7                           [1, 64]                   --\n",
       "├─Linear: 1-8                            [1, 1]                    65\n",
       "==========================================================================================\n",
       "Total params: 280,833\n",
       "Trainable params: 280,833\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.75\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.04\n",
       "Params size (MB): 1.12\n",
       "Estimated Total Size (MB): 1.16\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(Net(), input_data=torch.IntTensor(X_train[np.newaxis, 0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим датасеты:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataWrapper(Dataset):\n",
    "    def __init__(self, data, target):\n",
    "        self.data = torch.from_numpy(data)\n",
    "        self.target = torch.from_numpy(target)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "            \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(RANDOM_STATE)\n",
    "\n",
    "train_dataset = DataWrapper(X_train, df_train['label'].values)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = DataWrapper(X_val, df_val['label'].values)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем код сети. Учитывая дисбаланс классов, метрика accuracy нам  \n",
    "не подходит. Вместо неё будем использовать F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(epochs=5, embedding_dim=128, hidden_size=32, lr=1e-2, threshold=0.5, return_model=False):\n",
    "\n",
    "    torch.random.manual_seed(RANDOM_STATE)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    net = Net(vocab_size=MAX_WORDS, embedding_dim=embedding_dim, \n",
    "              out_channel=hidden_size, threshold=threshold).to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_losses = np.array([])\n",
    "        test_losses = np.array([])\n",
    "        tp, fp, tn, fn = 0, 0, 0, 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            net.train()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses = np.append(train_losses, loss.item())\n",
    "\n",
    "            net.eval()\n",
    "            outputs = torch.squeeze((net(inputs) > threshold).int())\n",
    "\n",
    "            tp += ((labels == 1) & (outputs == 1)).sum().item()\n",
    "            tn += ((labels == 0) & (outputs == 0)).sum().item()\n",
    "            fp += ((labels == 0) & (outputs == 1)).sum().item()\n",
    "            fn += ((labels == 1) & (outputs == 0)).sum().item()\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}]. ' \\\n",
    "              f'Loss: {train_losses.mean():.3f}. ' \\\n",
    "              f'F1-score: {f1_score:.3f}', end='. ')\n",
    "\n",
    "        tp, fp, tn, fn = 0, 0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, labels) in enumerate(val_loader):\n",
    "\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = net(inputs)\n",
    "\n",
    "                loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "                test_losses = np.append(test_losses, loss.item())\n",
    "\n",
    "                tp += ((labels == 1) & (torch.squeeze((outputs > threshold).int()) == 1)).sum()\n",
    "                tn += ((labels == 0) & (torch.squeeze((outputs > threshold).int()) == 0)).sum()\n",
    "                fp += ((labels == 0) & (torch.squeeze((outputs > threshold).int()) == 1)).sum()\n",
    "                fn += ((labels == 1) & (torch.squeeze((outputs > threshold).int()) == 0)).sum()\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "        print(f'Test loss: {test_losses.mean():.3f}. Test F1-score: {f1_score:.3f}. Precision: {precision:.3f}. Recall: {recall:.3f}')\n",
    "\n",
    "    print('Training is finished!')\n",
    "    if return_model:\n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель на 20 эпохах:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]. Loss: 0.291. F1-score: 0.032. Test loss: 0.198. Test F1-score: 0.022. Precision: 1.000. Recall: 0.011\n",
      "Epoch [2/20]. Loss: 0.204. F1-score: 0.120. Test loss: 0.173. Test F1-score: 0.247. Precision: 0.914. Recall: 0.143\n",
      "Epoch [3/20]. Loss: 0.170. F1-score: 0.394. Test loss: 0.164. Test F1-score: 0.437. Precision: 0.846. Recall: 0.295\n",
      "Epoch [4/20]. Loss: 0.148. F1-score: 0.541. Test loss: 0.160. Test F1-score: 0.523. Precision: 0.802. Recall: 0.388\n",
      "Epoch [5/20]. Loss: 0.136. F1-score: 0.608. Test loss: 0.160. Test F1-score: 0.522. Precision: 0.805. Recall: 0.386\n",
      "Epoch [6/20]. Loss: 0.121. F1-score: 0.677. Test loss: 0.163. Test F1-score: 0.560. Precision: 0.773. Recall: 0.440\n",
      "Epoch [7/20]. Loss: 0.116. F1-score: 0.708. Test loss: 0.172. Test F1-score: 0.559. Precision: 0.798. Recall: 0.431\n",
      "Epoch [8/20]. Loss: 0.108. F1-score: 0.721. Test loss: 0.179. Test F1-score: 0.570. Precision: 0.787. Recall: 0.446\n",
      "Epoch [9/20]. Loss: 0.102. F1-score: 0.746. Test loss: 0.184. Test F1-score: 0.583. Precision: 0.795. Recall: 0.460\n",
      "Epoch [10/20]. Loss: 0.098. F1-score: 0.752. Test loss: 0.191. Test F1-score: 0.589. Precision: 0.784. Recall: 0.471\n",
      "Epoch [11/20]. Loss: 0.091. F1-score: 0.780. Test loss: 0.199. Test F1-score: 0.590. Precision: 0.761. Recall: 0.482\n",
      "Epoch [12/20]. Loss: 0.091. F1-score: 0.795. Test loss: 0.203. Test F1-score: 0.582. Precision: 0.724. Recall: 0.487\n",
      "Epoch [13/20]. Loss: 0.085. F1-score: 0.812. Test loss: 0.213. Test F1-score: 0.586. Precision: 0.736. Recall: 0.487\n",
      "Epoch [14/20]. Loss: 0.084. F1-score: 0.820. Test loss: 0.219. Test F1-score: 0.585. Precision: 0.724. Recall: 0.491\n",
      "Epoch [15/20]. Loss: 0.081. F1-score: 0.830. Test loss: 0.242. Test F1-score: 0.583. Precision: 0.772. Recall: 0.469\n",
      "Epoch [16/20]. Loss: 0.080. F1-score: 0.845. Test loss: 0.246. Test F1-score: 0.586. Precision: 0.736. Recall: 0.487\n",
      "Epoch [17/20]. Loss: 0.078. F1-score: 0.850. Test loss: 0.257. Test F1-score: 0.584. Precision: 0.706. Recall: 0.498\n",
      "Epoch [18/20]. Loss: 0.072. F1-score: 0.862. Test loss: 0.276. Test F1-score: 0.584. Precision: 0.732. Recall: 0.487\n",
      "Epoch [19/20]. Loss: 0.073. F1-score: 0.865. Test loss: 0.288. Test F1-score: 0.576. Precision: 0.702. Recall: 0.489\n",
      "Epoch [20/20]. Loss: 0.073. F1-score: 0.872. Test loss: 0.298. Test F1-score: 0.566. Precision: 0.725. Recall: 0.464\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "train_nn(epochs=20, embedding_dim=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что модель быстро переобучается, несмотря на то, что у нас всего  \n",
    "один слой свёртки и один выходной полносвязный, а также два слоя дропаута.  \n",
    "Это связано с малым размером датасета. Сильное снижение количества каналов  \n",
    "свёртки приводит к тому, что модель просто перестаёт обучаться, 32 канала  \n",
    "в данном случае - более-менее оптимальное количество, найденное эмпирически.  \n",
    "Снижение размерности эмбеддингов снижает переобучение, но не устраняет его,  \n",
    "и, в целом, модель хуже показывает себя на тестовых данных.  \n",
    "По значениям лосса и метрики на тесте считаю, что оптимальное значение эпох -  \n",
    "7-9. Также имеем ввиду, что перед нами стоит задача выявления оскорбительных  \n",
    "твитов, а значит метрика Recall имеет важное значение. Обучим нашу модель  \n",
    "заново на 9 эпохах и снизим порог классификации, чтобы выявлять больше  \n",
    "оскорбительных твитов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/9]. Loss: 0.291. F1-score: 0.223. Test loss: 0.198. Test F1-score: 0.321. Precision: 0.333. Recall: 0.310\n",
      "Epoch [2/9]. Loss: 0.204. F1-score: 0.427. Test loss: 0.173. Test F1-score: 0.448. Precision: 0.398. Recall: 0.513\n",
      "Epoch [3/9]. Loss: 0.170. F1-score: 0.536. Test loss: 0.164. Test F1-score: 0.465. Precision: 0.388. Recall: 0.578\n",
      "Epoch [4/9]. Loss: 0.148. F1-score: 0.591. Test loss: 0.160. Test F1-score: 0.469. Precision: 0.383. Recall: 0.607\n",
      "Epoch [5/9]. Loss: 0.136. F1-score: 0.617. Test loss: 0.160. Test F1-score: 0.500. Precision: 0.434. Recall: 0.592\n",
      "Epoch [6/9]. Loss: 0.121. F1-score: 0.647. Test loss: 0.163. Test F1-score: 0.509. Precision: 0.421. Recall: 0.643\n",
      "Epoch [7/9]. Loss: 0.116. F1-score: 0.680. Test loss: 0.172. Test F1-score: 0.516. Precision: 0.442. Recall: 0.618\n",
      "Epoch [8/9]. Loss: 0.108. F1-score: 0.698. Test loss: 0.179. Test F1-score: 0.529. Precision: 0.456. Recall: 0.629\n",
      "Epoch [9/9]. Loss: 0.102. F1-score: 0.721. Test loss: 0.184. Test F1-score: 0.546. Precision: 0.486. Recall: 0.623\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "my_net = train_nn(epochs=9, threshold=0.25, return_model=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы находим почти 62% оскорбительных твитов, правда, точность  \n",
    "оставляет желать лучшего, и будет много \"ложных тревог\". Если бы у нас  \n",
    "было больше данных, то модель обучилась лучше. Проверим, как работает  \n",
    "предсказание модели на единичном примере:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, dtype=torch.int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_net.predict(X_val[np.newaxis, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
